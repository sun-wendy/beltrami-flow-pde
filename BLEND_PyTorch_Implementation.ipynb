{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Code implementation references:\n",
        "\n",
        "1) The paper \"Beltrami Flow and Neural Diffusion on Graphs\" by Chamberlain et al.\n",
        "\n",
        "2) https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial7/GNN_overview.html#PyTorch-Geometric"
      ],
      "metadata": {
        "id": "dDRnov6gsfle"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMdOaAR8s9dt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import networkx as nx\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "\n",
        "try:\n",
        "    import torch_geometric\n",
        "except ModuleNotFoundError:\n",
        "    # Installing torch geometric packages with specific CUDA+PyTorch version.\n",
        "    # See https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html for details\n",
        "    TORCH = torch.__version__.split('+')[0]\n",
        "    CUDA = 'cu' + torch.version.cuda.replace('.','')\n",
        "    !pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "    !pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "    !pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "    !pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "    !pip install torch-geometric\n",
        "    import torch_geometric\n",
        "import torch_geometric.nn as geom_nn\n",
        "import torch_geometric.data as geom_data\n",
        "import torch_geometric.datasets\n",
        "from torch_geometric.datasets import Planetoid\n",
        "\n",
        "# PyTorch Lightning\n",
        "try:\n",
        "    import pytorch_lightning as pl\n",
        "except ModuleNotFoundError:\n",
        "    !pip install --quiet pytorch-lightning>=1.4\n",
        "    import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
        "\n",
        "# Setting the seed\n",
        "pl.seed_everything(42)\n",
        "\n",
        "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5T4tM-W7tEuO"
      },
      "outputs": [],
      "source": [
        "# Download Cora dataset\n",
        "data_dir = \"./data\"\n",
        "\n",
        "# Split training, validation, and test data randomly\n",
        "dataset = Planetoid(root=data_dir, name='Cora', split='random')\n",
        "data = dataset[0]\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9IlWFLrStHGD"
      },
      "outputs": [],
      "source": [
        "# Get nodes, edges, and features\n",
        "print(\"# of Nodes: \", data.num_nodes)\n",
        "\n",
        "edges = data.edge_index\n",
        "print(\"\\nEdges:\\n\", edges)\n",
        "print(\"Shape of Edges: \", edges.size())\n",
        "\n",
        "features = data.x\n",
        "features_updated = torch.unsqueeze(features, 0)\n",
        "print(\"\\nFeatures:\\n\", features_updated)\n",
        "print(\"Shape of Features: \", features_updated.size())\n",
        "\n",
        "labels = data.y\n",
        "print(\"\\nLabels:\\n\", labels)\n",
        "print(\"Shape of Labels: \", labels.size())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the initial graph\n",
        "def construct_graph(edges):\n",
        "    edge_list = zip(edges[0], edges[1])\n",
        "    # Directed graph with edges going both directions\n",
        "    g = nx.DiGraph(edge_list)\n",
        "    return g\n",
        "\n",
        "G = construct_graph(edges)\n",
        "print(G)\n",
        "nx.draw(G)"
      ],
      "metadata": {
        "id": "WLALyUHYpdJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZWZ6KwetO4S"
      },
      "outputs": [],
      "source": [
        "# Beltrami Flow layer\n",
        "class BeltramiLayer(nn.Module):\n",
        "  def __init__(self, in_dim, out_dim, num_heads=1, concat_heads=True, alpha=0.2):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    in_dim (int) - input dimension\n",
        "    out_dim (int) - output dimension\n",
        "    num_heads (int) - # of heads, attention mechanism applied in parallel\n",
        "    concat_heads (bool) - if True, outputs of different heads concatenated rather than averaged\n",
        "    alpha (float) - negative slope of LeakyReLU activation\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.concat_heads = concat_heads\n",
        "\n",
        "    # If concatenate outputs of heads, output dimension should be a multiple of # of heads\n",
        "    if self.concat_heads:\n",
        "      assert out_dim % num_heads == 0\n",
        "      out_dim = out_dim // num_heads\n",
        "    \n",
        "    # Sub-modules and parameters\n",
        "    self.projection = nn.Linear(in_dim, out_dim*num_heads)\n",
        "    self.a = nn.Parameter(torch.Tensor(num_heads, 2*out_dim))\n",
        "    self.leakyrelu = nn.LeakyReLU(alpha)\n",
        "\n",
        "    # Xavier initialization\n",
        "    nn.init.xavier_uniform_(self.projection.weight.data, gain=1.414)\n",
        "    nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
        "\n",
        "  def forward(self, node_feats, edge_list, print_attn_probs=False, alpha=1):\n",
        "      \"\"\"\n",
        "      Parameters:\n",
        "          node_feats - input features of the node, shape = [1, batch_size, c_in]\n",
        "          edge_list - list of edges, shape = [2, 10556]\n",
        "          print_attn_probs - if True, attention weights are printed during forward pass (for debugging purposes)\n",
        "      \"\"\"\n",
        "      batch_size, num_nodes = node_feats.size(0), node_feats.size(1)\n",
        "      num_feats = node_feats.size(2)\n",
        "\n",
        "      # Apply linear layer, and sort nodes by head\n",
        "      node_feats = self.projection(node_feats)\n",
        "      node_feats = node_feats.view(batch_size, num_nodes, self.num_heads, -1)\n",
        "\n",
        "      edge_transposed = torch.transpose(edge_list, 0, 1)\n",
        "\n",
        "      # Calculate attention MLP output (independent for each head)\n",
        "      attn_matrix = torch.empty((num_nodes, num_nodes, num_feats))\n",
        "      print(attn_matrix.size())\n",
        "      \n",
        "      for edge in edge_transposed:\n",
        "        src_node = edge[0].item()\n",
        "        dest_node = edge[1].item()\n",
        "        diff = torch.subtract(node_feats[0][src_node], node_feats[0][dest_node])\n",
        "        attn = diff.apply_(lambda x: (1 / math.sqrt(1 + (alpha**2) * (x**2))))\n",
        "        attn_updated = torch.squeeze(attn)\n",
        "        attn_matrix[src_node][dest_node] = attn_updated\n",
        "      print(\"Attention matrix\\n\", attn_matrix)\n",
        "\n",
        "      # Weighted average of attention\n",
        "      attn_probs = F.softmax(attn_matrix, dim=2)\n",
        "      if print_attn_probs:\n",
        "          print(\"Attention probs\\n\", attn_probs.permute(0, 3, 1, 2))\n",
        "      node_feats = torch.einsum('bijh,bjhc->bihc', attn_probs, node_feats)\n",
        "\n",
        "      # If heads concatenated, we can do this by reshaping. Otherwise, take mean\n",
        "      if self.concat_heads:\n",
        "          node_feats = node_feats.reshape(batch_size, num_nodes, -1)\n",
        "      else:\n",
        "          node_feats = node_feats.mean(dim=2)\n",
        "\n",
        "      return node_feats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zx3juYO-aECO"
      },
      "outputs": [],
      "source": [
        "layer = BeltramiLayer(1433, 1433, num_heads=1)\n",
        "\n",
        "with torch.no_grad():\n",
        "  out_feats = layer(features_updated, edges, print_attn_probs=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BLEND network\n",
        "class BLENDModel(nn.Module):\n",
        "  def __init__(self, in_dim, hidden_dim, out_dim, num_layers=2, drop_rate=0.1, **kwargs):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    in_dim (int) - input dimension\n",
        "    hidden_dim (int) - dimension of hidden features\n",
        "    out_dim (int) - output dimension\n",
        "    num_layers (int) - # of hidden graph layers\n",
        "    drop_rate (float) - dropout rate to apply throughout the network\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    layers = []\n",
        "    in_channels, out_channels = in_dim, hidden_dim\n",
        "    for l_idx in range(num_layers-1):\n",
        "      layers += [\n",
        "          BeltramiLayer(in_dim=in_channels, out_dim=out_channels, **kwargs),\n",
        "          nn.ReLU(inplace=True),\n",
        "          nn.Dropout(drop_rate)\n",
        "      ]\n",
        "      in_channels = hidden_dim\n",
        "    layers += [\n",
        "        BeltramiLayer(in_dim=in_channels, out_dim=out_channels, **kwargs)\n",
        "    ]\n",
        "    self.layers = nn.ModuleList(layers)\n",
        "\n",
        "  def forward(self, x, edge_index):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    x (int) - input features per node\n",
        "    edge_index (list) - list of vertex index pairs representing the edges (PyTorch geometric notation)\n",
        "    \"\"\"\n",
        "    for l in self.layers:\n",
        "      # PyTorch geometric graph layers all inherit the MessagePassing class\n",
        "      if isinstance(l, geom_nn.MessagePassing):\n",
        "        x = l(x, edge_index)\n",
        "      else:\n",
        "        x = l(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "ePqIaYC8oa8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training, validation, & testing\n",
        "class NodeLevelBLEND(pl.LightningModule):\n",
        "  def __init__(self, **model_kwargs):\n",
        "    super().__init__()\n",
        "    # Save hyperparameters\n",
        "    self.save_hyperparameters()\n",
        "\n",
        "    self.model = BLENDModel(**model_kwargs)\n",
        "    self.loss_module = nn.CrossEntropyLoss()\n",
        "  \n",
        "  def forward(self, data, mode=\"train\"):\n",
        "    x, edge_index = data.x, data.edge_index\n",
        "    x = self.model(x, edge_index)\n",
        "\n",
        "    # Only calculate loss on the nodes corresponding to the mask\n",
        "    if mode == \"train\":\n",
        "      mask = data.train_mask\n",
        "    elif mode == \"val\":\n",
        "      mask = data.val_mask\n",
        "    elif mode == \"test\":\n",
        "      mask = data.test_mask\n",
        "    else:\n",
        "      assert False, f\"Unknown forward mode: {mode}\"\n",
        "    \n",
        "    loss = self.loss_module(x[mask], data.y[mask])\n",
        "    accuracy = (x[mask].argmax(dim=-1) == data.y[mask]).sum().float() / mask.sum()\n",
        "    return loss, accuracy\n",
        "  \n",
        "  def configure_optimizers(self):\n",
        "    # Use SGD here\n",
        "    optimizer = optim.SGD(self.parameters(), lr=0.1, momentum=0.9, weight_decay=2e-3)\n",
        "    return optimizer\n",
        "  \n",
        "  def training_step(self, batch, batch_idx):\n",
        "    loss, accuracy = self.forward(batch, mode=\"train\")\n",
        "    self.log('train_loss', loss)\n",
        "    self.log('train_accuracy', accuracy)\n",
        "    return loss\n",
        "  \n",
        "  def validation_step(self, batch, batch_idx):\n",
        "    _, accuracy = self.forward(batch, mode=\"val\")\n",
        "    self.log('val_accuracy', accuracy)\n",
        "\n",
        "  def test_step(self, batch, batch_idx):\n",
        "    _, accuracy = self.forward(batch, mode=\"test\")\n",
        "    self.log('test_accuracy', accuracy)"
      ],
      "metadata": {
        "id": "qkJUprgyobm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a training function\n",
        "def train_node_classifier(dataset, **model_kwargs):\n",
        "  pl.seed_everything(42)\n",
        "  node_data_loader = geom_data.DataLoader(dataset, batch_size=1)\n",
        "\n",
        "  # Create a PyTorch lightning trainer with generation callback\n",
        "  trainer = pl.Trainer(callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\")], \n",
        "                       accelerator=\"gpu\" if str(device).startswith(\"cuda\") else \"cpu\", \n",
        "                       devices=1, max_epochs=200, enable_progress_bar=False)\n",
        "  # Optional logging argument\n",
        "  trainer.logger._default_hp_metric = None\n",
        "\n",
        "  # Start model training\n",
        "  pl.seed_everything()\n",
        "  model = NodeLevelBLEND(in_dim=dataset.num_node_features, out_dim=dataset.num_classes, **model_kwargs)\n",
        "  trainer.fit(model, node_data_loader, node_data_loader)\n",
        "  model = NodeLevelBLEND.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
        "\n",
        "  # Test best model on the test set\n",
        "  test_result = trainer.test(model, node_data_loader, verbose=False)\n",
        "  batch = next(iter(node_data_loader))\n",
        "  batch = batch.to(model.device)\n",
        "  _, train_accuracy = model.forward(batch, mode=\"train\")\n",
        "  _, val_accuracy = model.forward(batch, mode=\"val\")\n",
        "  result = {\"train\": train_accuracy,\n",
        "            \"val\": val_accuracy,\n",
        "            \"test\": test_result[0]['test_accuracy']}\n",
        "  return model, result"
      ],
      "metadata": {
        "id": "bO8ynMCWohGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print test results\n",
        "def print_results(result_dict):\n",
        "    if \"train\" in result_dict:\n",
        "        print(f\"Train accuracy: {(100.0*result_dict['train']):4.2f}%\")\n",
        "    if \"val\" in result_dict:\n",
        "        print(f\"Val accuracy:   {(100.0*result_dict['val']):4.2f}%\")\n",
        "    print(f\"Test accuracy:  {(100.0*result_dict['test']):4.2f}%\")"
      ],
      "metadata": {
        "id": "P_SdSgdgolXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "node_blend_model, node_blend_result = train_node_classifier(dataset=dataset,\n",
        "                                                            hidden_dim=16,\n",
        "                                                            num_layers=2,\n",
        "                                                            drop_rate=0.1)\n",
        "\n",
        "print_results(node_blend_result)"
      ],
      "metadata": {
        "id": "QyRO6u6RonxE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "authorship_tag": "ABX9TyO0MTlH+czF59E5cerHdUsB"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}